{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Network in Python\n",
    "\n",
    "Below, I explore how to create and train a three-layer neural network in Python, making use of the NumPy linear algebra library. I've broken my code into the following sections:\n",
    "\n",
    "- __Activation Function__\n",
    "- __Training Data__\n",
    "- __Defining Alpha and Size of Hidden Layer__\n",
    "- __Weights__\n",
    "- __Training__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "First, we import `numpy` to be able to perform linear algebra operations. Then, we define the logistic function (sigmoid curve), which is key to creating non-linear hypotheses from our data. The logistic function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logistic Function Equation](logistic-formula.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # Linear algebra library\n",
    "import matplotlib.pyplot as plt # Charting library\n",
    "\n",
    "# Defining the logistic function (sigmoid curve)\n",
    "# This is our activation function\n",
    "def sigmoid(x, derive=False):\n",
    "    if(derive==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function converts any number to a value between 0 and 1, which can be used as probabilities to train our neural network. We can also use this method to retrieve the derivative of the logistic function, which is useful for the backpropagation process in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "Next, we'll set up our training data. For training inputs, we have a matrix of four rows and three columns. For training outputs, we have a matrix of four rows and a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training input data\n",
    "x = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])\n",
    "\n",
    "# Training output data\n",
    "y = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Alpha and Size of Hidden Layer\n",
    "\n",
    "The alpha parameter allows us to use gradient descent to modify the size of updates made to the weights. As you'll see in the output of the training process, using the right alpha parameter allows us to optimize our weights so that we achieve results that better fit our data.\n",
    "\n",
    "Parametrizing the size of the hidden layer allows us to increase the search space of our neural network, and converge to a global minimum faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alpha parameter for gradient descent\n",
    "alpha = 10\n",
    "\n",
    "# Parametrized size of hidden layer\n",
    "hiddenSize = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Weights\n",
    "\n",
    "We need to initialize random weights for our neural network. We only have three layers in this neural network (input, a single hidden layer and output). The first layer has three input units, and the third layer has a single output unit. Since we want to connect every node in the first layer to every node in the second layer, we need a matrix of three rows and a number of columns which matches the number of rows in our hidden layer. Our hidden layer only needs a single column to connect to our output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WEIGHTS\n",
    "# Seeding our random number generation\n",
    "np.random.seed(1)\n",
    "    \n",
    "# Initialize weights randomly\n",
    "# Best practice is to initialize weights with a mean of 0\n",
    "weights_1 = 2*np.random.random((3,hiddenSize))-1\n",
    "weights_2 = 2*np.random.random((hiddenSize,1))-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In this for-loop, we are performing simultaneous training of our neural network across all examples (4) that we loaded earlier into the variable X. \n",
    "\n",
    "The first step of the training process involves attempting prediction of output, given our training example inputs. \n",
    "\n",
    "Then we compare our prediction of output to the actual training example output. The adjustment process involves multiplying these error vales by the slope of the logistic function at our prediction values. Take a look at a sample chart of a logistic function below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11799f780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_values = np.arange(-10., 10., 0.2)\n",
    "logistic_values = []\n",
    "for item in demo_values:\n",
    "    logistic_values.append(1/(1+np.exp(-item)))\n",
    "plt.plot(demo_values,logistic_values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our predictions are more confident, they will be at either the lower or higher end of the logistic curve. This means that our adjustments will be smaller, because we are incrementing weights by a number which is the result of multiplying the error value by the slope of the logistic curve at the point of our prediction. \n",
    "\n",
    "The slope of the logistic curve is closer to zero at both the lower and higher ends. There is likely to be a __small__ amount of error for high confidence predictions, so they only need to be adjusted by a __small__ amount.\n",
    "\n",
    "However, if our predictions are less confident, they will be closer to the center of the logistic curve, where the slope reaches its maximum value of 1. Therefore, we will make __greater__ adjustments when our model is __more unsure__ about the predictions it makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with Alpha:10\n",
      "Error after 0 iterations: 0.496439922501\n",
      "Error after 10000 iterations: 0.00225627779723\n",
      "Error after 20000 iterations: 0.00153822414655\n",
      "Error after 30000 iterations: 0.00123497929352\n",
      "Error after 40000 iterations: 0.00105841214612\n",
      "Error after 50000 iterations: 0.000939718817582\n"
     ]
    }
   ],
   "source": [
    "print (\"\\nTraining with Alpha:\" + str(alpha))\n",
    "       \n",
    "for iter in range(60000):\n",
    "    \n",
    "    # FORWARD PROPAGATION\n",
    "    \n",
    "    # Set first layer of neural network equal to our training input data\n",
    "    l0 = x\n",
    "    \n",
    "    # Applying the logistic function to the result of \n",
    "    # the dot product between the first layer of the neural network and the weights\n",
    "    l1 = sigmoid(np.dot(l0, weights_1)) \n",
    "    \n",
    "    # Applying the logisitic function to the result of\n",
    "    # the dot product again for the second layer of the neural network and the weights\n",
    "    l2 = sigmoid(np.dot(l1, weights_2))\n",
    "    \n",
    "    # ERROR CALCULATION/BACKPROPAGATION\n",
    "    \n",
    "    # By subtracting our hypothesis from the training output example\n",
    "    # we get an error calculation: y - h(x) (familiar from cost function)\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (iter% 10000) == 0:\n",
    "        print (\"Error after \" + str(iter) + \" iterations: \"+ str(np.mean(np.abs(l2_error))))\n",
    "    \n",
    "    # Multiplying the error amount by the slope of the logistic function\n",
    "    # at our second layer values, gives us a delta value we can use to update\n",
    "    # our weights\n",
    "    l2_delta = l2_error * sigmoid(l2, derive=True)\n",
    "    \n",
    "    # We can only calculate error values for our first layer once we have\n",
    "    # already calculated error for the second layer. Here, we're calculating\n",
    "    # how much each value in the first layer contributes to error in the second layer\n",
    "    l1_error = np.dot(l2_delta, weights_2.T)\n",
    "    \n",
    "    # Once again, we'll update the error using the slope of the logistic function\n",
    "    # at our first layer values - to make sure we are updating \"in the right direction\"\n",
    "    l1_delta = l1_error * sigmoid(l1, derive=True)\n",
    "    \n",
    "    # UPDATE WEIGHTS\n",
    "    \n",
    "    # Increment weight values by the result of the dot product \n",
    "    # between our layer values and backpropagation delta values\n",
    "    # Scale our increments by alpha (gradient descent)\n",
    "    weights_2 += alpha * np.dot(l1.T, l2_delta)\n",
    "    weights_1 += alpha * np.dot(l0.T, l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
